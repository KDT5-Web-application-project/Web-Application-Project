{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchinfo import summary\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdp\\AppData\\Local\\Temp\\ipykernel_11352\\188211148.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file = pd.read_csv('./data/encoded_data_v2.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158910</th>\n",
       "      <td>158910</td>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>A man in shorts and a hawaiian shirt leans ove...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158911</th>\n",
       "      <td>158911</td>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>A young man hanging over the side of a boat  w...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>276</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158912</th>\n",
       "      <td>158912</td>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>A man is leaning off of the side of a blue and...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>314</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158913</th>\n",
       "      <td>158913</td>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A man riding a small boat in a harbor  with fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>139</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158914</th>\n",
       "      <td>158914</td>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>A man on a moored blue and white boat with hil...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6294</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0     image_name comment_number  \\\n",
       "158910      158910  998845445.jpg              0   \n",
       "158911      158911  998845445.jpg              1   \n",
       "158912      158912  998845445.jpg              2   \n",
       "158913      158913  998845445.jpg              3   \n",
       "158914      158914  998845445.jpg              4   \n",
       "\n",
       "                                                  comment  0   1   2    3  \\\n",
       "158910  A man in shorts and a hawaiian shirt leans ove...  0   2   0   89   \n",
       "158911  A young man hanging over the side of a boat  w...  0  10   2  276   \n",
       "158912  A man is leaning off of the side of a blue and...  0   2   0  314   \n",
       "158913  A man riding a small boat in a harbor  with fo...  0   2  52    0   \n",
       "158914  A man on a moored blue and white boat with hil...  0   2   0    0   \n",
       "\n",
       "           4    5  ...  68  69  70  71  72  73  74  75  76  77  \n",
       "158910     0    0  ...   1   1   1   1   1   1   1   1   1   1  \n",
       "158911     0    0  ...   1   1   1   1   1   1   1   1   1   1  \n",
       "158912     0    0  ...   1   1   1   1   1   1   1   1   1   1  \n",
       "158913    45  139  ...   1   1   1   1   1   1   1   1   1   1  \n",
       "158914  6294   11  ...   1   1   1   1   1   1   1   1   1   1  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pd.read_csv('./data/encoded_data_v2.csv')\n",
    "file.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for k in range(file.shape[0]):\n",
    "    sample = []\n",
    "    img_dir = './data/flickr30k_images/'\n",
    "    img_file = img_dir + file.iloc[k][1] \n",
    "    sample.append(img_file)\n",
    "\n",
    "    capt = []\n",
    "    for idx in range(78):\n",
    "        capt.append(file.iloc[k][idx+4])\n",
    "        # print(file.iloc[k][idx+4])\n",
    "\n",
    "    sample.append(capt)\n",
    "    data.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/flickr30k_images/1000092795.jpg', [37, 10, 288, 0, 2075, 81, 147, 0, 0, 121, 0, 276, 0, 0, 0, 438, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "78\n",
      "\n",
      "['./data/flickr30k_images/1000092795.jpg', [37, 10, 7, 662, 0, 33, 55, 273, 1379, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "78\n",
      "\n",
      "['./data/flickr30k_images/1000092795.jpg', [37, 18, 0, 28, 209, 0, 16, 0, 0, 438, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in range(3):\n",
    "    print(data[k])\n",
    "    print(len(data[k][1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 데이터셋 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 이미지와 캡션을 로드하고 전처리하여 반환\n",
    "        image_path, caption = self.data[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        caption = torch.LongTensor(caption)\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158915\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 인스턴스 생성\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "dataset = CustomDataset(data, transform=transform)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.0665, -2.0323, -2.0323,  ...,  0.8276,  1.2214,  1.4269],\n",
      "         [-2.0323, -2.0323, -1.9809,  ...,  0.4851, -0.3712, -0.1486],\n",
      "         [-1.9638, -2.0323, -2.0323,  ..., -0.7137, -0.5767, -0.3369],\n",
      "         ...,\n",
      "         [-0.7308, -0.7479, -0.4054,  ...,  0.1939,  0.1939,  0.1939],\n",
      "         [-0.4568, -0.1828, -0.0801,  ..., -0.0287,  0.2624,  0.3138],\n",
      "         [ 0.4851,  0.2282,  0.2624,  ...,  0.2796,  0.4166,  0.3481]],\n",
      "\n",
      "        [[-1.9482, -1.9307, -1.8957,  ...,  1.6408,  1.9559,  2.1835],\n",
      "         [-1.9132, -1.9307, -1.8431,  ...,  1.3256,  0.6254,  0.9580],\n",
      "         [-1.7906, -1.8957, -1.8957,  ...,  0.2577,  0.4853,  0.7479],\n",
      "         ...,\n",
      "         [-0.3550, -0.3550,  0.0651,  ...,  0.9755,  0.9930,  0.9930],\n",
      "         [ 0.0476,  0.2052,  0.4678,  ...,  0.9405,  1.1155,  1.0805],\n",
      "         [ 1.0105,  1.0455,  0.8880,  ...,  1.0280,  1.0980,  1.0630]],\n",
      "\n",
      "        [[-1.7173, -1.7173, -1.6999,  ...,  1.6465,  2.1694,  2.3611],\n",
      "         [-1.7173, -1.7173, -1.6476,  ...,  1.0714, -0.2707, -0.1835],\n",
      "         [-1.6302, -1.6999, -1.6999,  ..., -0.8807, -1.0550, -1.2293],\n",
      "         ...,\n",
      "         [-0.7936, -0.7238, -0.3753,  ..., -0.0790, -0.0092, -0.0441],\n",
      "         [-0.5670, -0.2707, -0.1487,  ..., -0.2532,  0.0779,  0.1651],\n",
      "         [ 0.2696, -0.0092,  0.0605,  ...,  0.0082,  0.2696,  0.1476]]])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([  37,   10,  288,    0, 2075,   81,  147,    0,    0,  121,    0,  276,\n",
      "           0,    0,    0,  438,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for img, capt in dataset:\n",
    "    print(img)\n",
    "    print(type(img))\n",
    "    print(capt)\n",
    "    print(type(capt))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111241 15891 31783\n"
     ]
    }
   ],
   "source": [
    "# dataset에서 train, valid, test를 나누기 \n",
    "seed_gen = torch.Generator().manual_seed(42)\n",
    "tr, val, ts = 0.7,0.1,0.2\n",
    "trainDS, validDS, testDS = random_split(dataset, [tr, val, ts], generator=seed_gen)\n",
    "print(len(trainDS), len(validDS), len(testDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434 62 124\n"
     ]
    }
   ],
   "source": [
    "# dataloader 생성\n",
    "batch_size = 256\n",
    "train_dl = DataLoader(trainDS, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "valid_dl = DataLoader(validDS, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "test_dl = DataLoader(testDS, batch_size = batch_size, shuffle=True, drop_last = True)\n",
    "print(len(train_dl), len(valid_dl), len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,    0,    7,  ...,    1,    1,    1],\n",
      "        [ 130,    2,    0,  ...,    1,    1,    1],\n",
      "        [1443,  233,    0,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   0,   10,   15,  ...,    1,    1,    1],\n",
      "        [   0,  222,  263,  ...,    1,    1,    1],\n",
      "        [   0,  489,    0,  ...,    1,    1,    1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for img, capt in train_dl:\n",
    "    print(capt)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1개의 배치 안에 있는 이미지 확인\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def show_batch(dl):\n",
    "    \"\"\"Plot images grid of single batch\"\"\"\n",
    "    for (images, labels) in dl:\n",
    "        fig,ax = plt.subplots(figsize = (16,12))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
    "        break\n",
    "        \n",
    "# show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 클래스 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Captioning with Pytorch\n",
    "- 필요한 모델 : CNN & RNN \n",
    "- 인코딩용 : CNN => Resnet\n",
    "- 디코딩용 : RNN => LSTM\n",
    "- CNN에서 나온 결과물을 LSTM에 연결 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩용 CNN 모델 생성 : RESNET18 (가중치O)\n",
    "\n",
    "resnet = models.resnet18()\n",
    "# 전결합층 변경\n",
    "resnet.fc = nn.Linear(in_features = 512, out_features = 1)\n",
    "\n",
    "# 모델의 합성곱층 가중치 고정 (완전 연결층은 학습시켜야함)\n",
    "for name, param in resnet.named_parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in resnet.fc.named_parameters():\n",
    "    param.requires_grad = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코딩용 RNN 모델 생성 : LSTM\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "# 패딩된 시퀀스를 실제 데이터 길이에 맞게 패킹하여 효율적인 연산을 수행할 수 있게 해줌\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers = 1):\n",
    "        super(decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        embedding = self.embed(captions)\n",
    "        embedding = torch.cat((features.unsqueeze(2), embedding.unsqueeze(1)),1)\n",
    "        packed = pack_padded_sequence(embedding, lengths, batch_first = True)\n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- class로 만든거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩용 CNN 모델 생성 : RESNET18 (가중치O)\n",
    "\n",
    "# res_model = models.resnet18(weights = ( \"ResNet18_Weights.DEFAULT\"))\n",
    "# 전결합층 변경\n",
    "# res_model.fc = nn.Linear(in_features = 512, out_features = 1)\n",
    "\n",
    "\n",
    "# class encoderCNN(nn.Module):\n",
    "#     def __init__(self, embed_size):\n",
    "#         super(encoderCNN, self).__init_()\n",
    "#         self.resnet = models.resnet18(weights = ( \"ResNet18_Weights.DEFAULT\"))\n",
    "#         # 전이학습 모델의 전결합층 변경\n",
    "#         self.resent.fc = nn.Linear(self.resenet.fc.in_features, embed_size)\n",
    "\n",
    "#         self.dropout = nn.Dropout(0.5)  # 흠,, 필요할까?\n",
    "#         self.relu = nn.Relu()\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         features = self.resnet(x)\n",
    "        \n",
    "#         # 모델의 합성곱층 가중치 고정 (완전 연결층은 학습시켜야함)\n",
    "#         for name, param in self.resnet.named_parameters():\n",
    "#             param.requires_grad = False\n",
    "#         for name, param in self.resnet.fc.named_parameters():\n",
    "#             param.requires_grad = True \n",
    "\n",
    "#         result = self.relu(features)\n",
    "\n",
    "#         return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코딩용 RNN 모델 생성 : LSTM\n",
    "\n",
    "# class decoderRNN(nn.Module):\n",
    "#     def __init__ (self, embed_size, vocab_size, hidden_size, num_layers):\n",
    "#         super(decoderRNN, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "#         self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "#         self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "#         self.dropout = nn.Dropout(0.5) # 흠 필요함?\n",
    "    \n",
    "#     def forward(self, features, caption):\n",
    "#         # embeddings = self.dropout(self.embedding(caption))   # 왜 dropout?\n",
    "#         embeddings = self.embedding(caption)\n",
    "#         embeddings = torch.cat((features.unsqueeze(0), embeddings), dim = 0)\n",
    "#         hiddens, _ = self.lstm(embeddings)\n",
    "#         outputs = self.linear(hiddens)\n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 과 RNN을 연결시키자\n",
    "\n",
    "# class CNN2RNN(nn.Module):\n",
    "#     def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n",
    "#         super(CNN2RNN, self).__init__()\n",
    "#         self.encoderCNN = encoderCNN(embed_size)\n",
    "#         self.decoderRNN = decoderRNN(embed_size, vocab_size, hidden_size, num_layers)\n",
    "    \n",
    "#     def captionImage(self, image, vocabulary, maxlength = 50):\n",
    "#         result_caption = []\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             x = self.encoderCNN(image).unsqueeze(0)\n",
    "#             states = None\n",
    "\n",
    "#             for _ in range(maxlength) :\n",
    "#                 hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "#                 output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
    "#                 predicted = output.argmax(1)\n",
    "#                 print(predicted.shape)\n",
    "\n",
    "#                 result_caption.append(predicted.item())\n",
    "#                 x = self.decoderRNN.embedding(output).unsqueeze(0)\n",
    "\n",
    "#                 if vocabulary.itos[predicted.item()] == \"<EOS>\" :\n",
    "#                     break\n",
    "        \n",
    "#         return [vocabulary.itos[i] for i in result_caption]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습을 위한 하이퍼파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = 21652  # 단어사전 크기\n",
    "num_layers = 1\n",
    "model = decoder(embed_size, hidden_size, 21653, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 17,  0, 48,  0,  0, 69,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1])\n",
      "74\n",
      "[74, 73, 73, 73, 71, 73, 72, 70, 74, 71, 73, 68, 73, 73, 73, 73, 69, 76, 72, 76, 75, 72, 75, 71, 66, 75, 71, 71, 69, 73, 74, 72, 71, 69, 71, 72, 72, 73, 68, 77, 76, 74, 73, 68, 75, 72, 73, 75, 77, 72, 74, 72, 73, 73, 70, 72, 75, 73, 76, 76, 73, 64, 68, 77, 73, 71, 73, 75, 75, 73, 73, 71, 76, 77, 75, 72, 73, 75, 75, 73, 70, 67, 71, 72, 74, 74, 73, 75, 71, 78, 72, 70, 74, 76, 76, 73, 74, 74, 71, 73, 75, 74, 73, 71, 73, 72, 70, 73, 71, 72, 73, 75, 77, 76, 73, 72, 70, 74, 62, 71, 72, 76, 73, 72, 76, 76, 72, 70, 70, 74, 75, 75, 75, 66, 71, 77, 70, 77, 74, 71, 76, 73, 70, 72, 73, 77, 70, 70, 71, 75, 75, 73, 73, 70, 76, 74, 73, 69, 74, 75, 74, 73, 75, 77, 75, 75, 72, 72, 70, 72, 75, 73, 76, 75, 75, 74, 72, 68, 72, 66, 68, 74, 77, 75, 74, 76, 73, 75, 74, 68, 77, 74, 74, 73, 67, 74, 72, 74, 74, 69, 75, 71, 72, 71, 72, 73, 72, 73, 73, 73, 75, 73, 73, 63, 69, 75, 77, 67, 72, 75, 71, 71, 72, 70, 71, 76, 75, 75, 68, 71, 73, 71, 75, 72, 76, 76, 72, 71, 73, 70, 75, 73, 71, 72, 76, 76, 63, 76, 74, 75, 74, 75, 70, 75, 70, 75]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([74., 73., 73., 73., 71., 73., 72., 70., 74., 71., 73., 68., 73., 73.,\n",
       "         73., 73., 69., 76., 72., 76., 75., 72., 75., 71., 66., 75., 71., 71.,\n",
       "         69., 73., 74., 72., 71., 69., 71., 72., 72., 73., 68., 77., 76., 74.,\n",
       "         73., 68., 75., 72., 73., 75., 77., 72., 74., 72., 73., 73., 70., 72.,\n",
       "         75., 73., 76., 76., 73., 64., 68., 77., 73., 71., 73., 75., 75., 73.,\n",
       "         73., 71., 76., 77., 75., 72., 73., 75., 75., 73., 70., 67., 71., 72.,\n",
       "         74., 74., 73., 75., 71., 78., 72., 70., 74., 76., 76., 73., 74., 74.,\n",
       "         71., 73., 75., 74., 73., 71., 73., 72., 70., 73., 71., 72., 73., 75.,\n",
       "         77., 76., 73., 72., 70., 74., 62., 71., 72., 76., 73., 72., 76., 76.,\n",
       "         72., 70., 70., 74., 75., 75., 75., 66., 71., 77., 70., 77., 74., 71.,\n",
       "         76., 73., 70., 72., 73., 77., 70., 70., 71., 75., 75., 73., 73., 70.,\n",
       "         76., 74., 73., 69., 74., 75., 74., 73., 75., 77., 75., 75., 72., 72.,\n",
       "         70., 72., 75., 73., 76., 75., 75., 74., 72., 68., 72., 66., 68., 74.,\n",
       "         77., 75., 74., 76., 73., 75., 74., 68., 77., 74., 74., 73., 67., 74.,\n",
       "         72., 74., 74., 69., 75., 71., 72., 71., 72., 73., 72., 73., 73., 73.,\n",
       "         75., 73., 73., 63., 69., 75., 77., 67., 72., 75., 71., 71., 72., 70.,\n",
       "         71., 76., 75., 75., 68., 71., 73., 71., 75., 72., 76., 76., 72., 71.,\n",
       "         73., 70., 75., 73., 71., 72., 76., 76., 63., 76., 74., 75., 74., 75.,\n",
       "         70., 75., 70., 75.]),\n",
       " 256)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for image, caption in train_dl:\n",
    "    print(caption[0])\n",
    "    a = np.array(caption[0])\n",
    "\n",
    "    print(len(np.nonzero(a)[0]))\n",
    "    num_list = []\n",
    "    for ind in range(len(image)):\n",
    "        a = np.array(caption[ind])\n",
    "        num_list.append(len(np.nonzero(a)[0]))\n",
    "    print(num_list)\n",
    "    break\n",
    "\n",
    "torch.Tensor(num_list), len(num_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습, 검증, 테스트 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1 but got size 256 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 55\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# print(f\"[TOTAL Train Loss] ==> {total_loss}\")\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# print(f\"ACC : {total_acc}  Precision : {total_precision}  Recall : {total_recall} F1score : {total_f1score}\")\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss, total_acc, total_precision, total_recall, total_f1score\n\u001b[1;32m---> 55\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[121], line 26\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m     23\u001b[0m features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mview(features\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 예측\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 역전파\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[113], line 15\u001b[0m, in \u001b[0;36mdecoder.forward\u001b[1;34m(self, features, captions, lengths)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, features, captions, lengths):\n\u001b[0;32m     14\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(captions)\n\u001b[1;32m---> 15\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     packed \u001b[38;5;241m=\u001b[39m pack_padded_sequence(embedding, lengths, batch_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m     hiddens, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(packed)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 1 but got size 256 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torchmetrics.functional as metrics\n",
    "\n",
    "def training(dataloader):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1score_list = []\n",
    "\n",
    "    for images, captions in dataloader:        \n",
    "        num_list = []\n",
    "        for ind in range(len(image)):\n",
    "            a = np.array(caption[ind])\n",
    "            num_list.append(len(np.nonzero(a)[0]))\n",
    "    \n",
    "        lengths = torch.Tensor(num_list)     #근데 이게 배치사이즈가 맞나??\n",
    "        \n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first = True, enforce_sorted=False)[0]\n",
    "\n",
    "        # 이미지 특성 추출\n",
    "        features = resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        # 예측\n",
    "        outputs = model(features, captions, lengths)\n",
    "        print(outputs)\n",
    "        # 역전파\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 정확도\n",
    "        acc = metrics.accuracy(outputs, label, task = 'multiclass') \n",
    "        precision = metrics.precision(outputs, label, task = 'multiclass')\n",
    "        recall = metrics.recall(outputs, label, task = 'multiclass')\n",
    "        f1score = metrics.f1_score(outputs, label, task = 'multiclass')\n",
    "        \n",
    "        loss_list.append(loss)\n",
    "        acc_list.append(acc)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1score_list.append(f1score)\n",
    "        \n",
    "    total_loss = sum(loss_list) / len(loss_list)\n",
    "    total_acc = sum(acc_list) / len(acc_list)\n",
    "    total_precision = sum(precision_list) / len(precision_list)\n",
    "    total_recall = sum(recall_list)/len(recall_list)\n",
    "    total_f1score = sum(f1score_list) / len(f1score_list)\n",
    "    # print(f\"[TOTAL Train Loss] ==> {total_loss}\")\n",
    "    # print(f\"ACC : {total_acc}  Precision : {total_precision}  Recall : {total_recall} F1score : {total_f1score}\")\n",
    "    return total_loss, total_acc, total_precision, total_recall, total_f1score\n",
    "\n",
    "training(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
