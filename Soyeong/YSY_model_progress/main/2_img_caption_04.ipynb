{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchinfo import summary\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdp\\AppData\\Local\\Temp\\ipykernel_18780\\188211148.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file = pd.read_csv('./data/encoded_data_v2.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158910</th>\n",
       "      <td>158910</td>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>A man in shorts and a hawaiian shirt leans ove...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158911</th>\n",
       "      <td>158911</td>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>A young man hanging over the side of a boat  w...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>276</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158912</th>\n",
       "      <td>158912</td>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>A man is leaning off of the side of a blue and...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>314</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158913</th>\n",
       "      <td>158913</td>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A man riding a small boat in a harbor  with fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>139</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158914</th>\n",
       "      <td>158914</td>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>A man on a moored blue and white boat with hil...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6294</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0     image_name comment_number  \\\n",
       "158910      158910  998845445.jpg              0   \n",
       "158911      158911  998845445.jpg              1   \n",
       "158912      158912  998845445.jpg              2   \n",
       "158913      158913  998845445.jpg              3   \n",
       "158914      158914  998845445.jpg              4   \n",
       "\n",
       "                                                  comment  0   1   2    3  \\\n",
       "158910  A man in shorts and a hawaiian shirt leans ove...  0   2   0   89   \n",
       "158911  A young man hanging over the side of a boat  w...  0  10   2  276   \n",
       "158912  A man is leaning off of the side of a blue and...  0   2   0  314   \n",
       "158913  A man riding a small boat in a harbor  with fo...  0   2  52    0   \n",
       "158914  A man on a moored blue and white boat with hil...  0   2   0    0   \n",
       "\n",
       "           4    5  ...  68  69  70  71  72  73  74  75  76  77  \n",
       "158910     0    0  ...   1   1   1   1   1   1   1   1   1   1  \n",
       "158911     0    0  ...   1   1   1   1   1   1   1   1   1   1  \n",
       "158912     0    0  ...   1   1   1   1   1   1   1   1   1   1  \n",
       "158913    45  139  ...   1   1   1   1   1   1   1   1   1   1  \n",
       "158914  6294   11  ...   1   1   1   1   1   1   1   1   1   1  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pd.read_csv('./data/encoded_data_v2.csv')\n",
    "file.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for k in range(file.shape[0]):\n",
    "    sample = []\n",
    "    img_dir = './data/flickr30k_images/'\n",
    "    img_file = img_dir + file.iloc[k][1] \n",
    "    sample.append(img_file)\n",
    "\n",
    "    capt = []\n",
    "    for idx in range(78):\n",
    "        capt.append(file.iloc[k][idx+4])\n",
    "        # print(file.iloc[k][idx+4])\n",
    "\n",
    "    sample.append(capt)\n",
    "    data.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/flickr30k_images/1000092795.jpg', [37, 10, 288, 0, 2075, 81, 147, 0, 0, 121, 0, 276, 0, 0, 0, 438, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "78\n",
      "\n",
      "['./data/flickr30k_images/1000092795.jpg', [37, 10, 7, 662, 0, 33, 55, 273, 1379, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "78\n",
      "\n",
      "['./data/flickr30k_images/1000092795.jpg', [37, 18, 0, 28, 209, 0, 16, 0, 0, 438, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in range(3):\n",
    "    print(data[k])\n",
    "    print(len(data[k][1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 데이터셋 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 이미지와 캡션을 로드하고 전처리하여 반환\n",
    "        image_path, caption = self.data[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        caption = torch.LongTensor(caption)\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158915\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 인스턴스 생성\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "dataset = CustomDataset(data, transform=transform)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.0665, -2.0323, -2.0323,  ...,  0.8276,  1.2214,  1.4269],\n",
      "         [-2.0323, -2.0323, -1.9809,  ...,  0.4851, -0.3712, -0.1486],\n",
      "         [-1.9638, -2.0323, -2.0323,  ..., -0.7137, -0.5767, -0.3369],\n",
      "         ...,\n",
      "         [-0.7308, -0.7479, -0.4054,  ...,  0.1939,  0.1939,  0.1939],\n",
      "         [-0.4568, -0.1828, -0.0801,  ..., -0.0287,  0.2624,  0.3138],\n",
      "         [ 0.4851,  0.2282,  0.2624,  ...,  0.2796,  0.4166,  0.3481]],\n",
      "\n",
      "        [[-1.9482, -1.9307, -1.8957,  ...,  1.6408,  1.9559,  2.1835],\n",
      "         [-1.9132, -1.9307, -1.8431,  ...,  1.3256,  0.6254,  0.9580],\n",
      "         [-1.7906, -1.8957, -1.8957,  ...,  0.2577,  0.4853,  0.7479],\n",
      "         ...,\n",
      "         [-0.3550, -0.3550,  0.0651,  ...,  0.9755,  0.9930,  0.9930],\n",
      "         [ 0.0476,  0.2052,  0.4678,  ...,  0.9405,  1.1155,  1.0805],\n",
      "         [ 1.0105,  1.0455,  0.8880,  ...,  1.0280,  1.0980,  1.0630]],\n",
      "\n",
      "        [[-1.7173, -1.7173, -1.6999,  ...,  1.6465,  2.1694,  2.3611],\n",
      "         [-1.7173, -1.7173, -1.6476,  ...,  1.0714, -0.2707, -0.1835],\n",
      "         [-1.6302, -1.6999, -1.6999,  ..., -0.8807, -1.0550, -1.2293],\n",
      "         ...,\n",
      "         [-0.7936, -0.7238, -0.3753,  ..., -0.0790, -0.0092, -0.0441],\n",
      "         [-0.5670, -0.2707, -0.1487,  ..., -0.2532,  0.0779,  0.1651],\n",
      "         [ 0.2696, -0.0092,  0.0605,  ...,  0.0082,  0.2696,  0.1476]]])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([  37,   10,  288,    0, 2075,   81,  147,    0,    0,  121,    0,  276,\n",
      "           0,    0,    0,  438,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for img, capt in dataset:\n",
    "    print(img)\n",
    "    print(type(img))\n",
    "    print(capt)\n",
    "    print(type(capt))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111241 15891 31783\n"
     ]
    }
   ],
   "source": [
    "# dataset에서 train, valid, test를 나누기 \n",
    "seed_gen = torch.Generator().manual_seed(42)\n",
    "tr, val, ts = 0.7,0.1,0.2\n",
    "trainDS, validDS, testDS = random_split(dataset, [tr, val, ts], generator=seed_gen)\n",
    "print(len(trainDS), len(validDS), len(testDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434 62 124\n"
     ]
    }
   ],
   "source": [
    "# dataloader 생성\n",
    "batch_size = 256\n",
    "train_dl = DataLoader(trainDS, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "valid_dl = DataLoader(validDS, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "test_dl = DataLoader(testDS, batch_size = batch_size, shuffle=True, drop_last = True)\n",
    "print(len(train_dl), len(valid_dl), len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   6,  409,    0,  ...,    1,    1,    1],\n",
      "        [   3,    0,  237,  ...,    1,    1,    1],\n",
      "        [   0,  413, 4629,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   0,   17,    0,  ...,    1,    1,    1],\n",
      "        [   0,   15,    0,  ...,    1,    1,    1],\n",
      "        [1196,    2,  336,  ...,    1,    1,    1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for img, capt in train_dl:\n",
    "    print(capt)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1개의 배치 안에 있는 이미지 확인\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def show_batch(dl):\n",
    "    \"\"\"Plot images grid of single batch\"\"\"\n",
    "    for (images, labels) in dl:\n",
    "        fig,ax = plt.subplots(figsize = (16,12))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
    "        break\n",
    "        \n",
    "# show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클래스 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Captioning with Pytorch\n",
    "- 필요한 모델 : CNN & RNN \n",
    "- 인코딩용 : CNN => Resnet\n",
    "- 디코딩용 : RNN => LSTM\n",
    "- CNN에서 나온 결과물을 LSTM에 연결 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 인코딩용 CNN 모델 생성 : RESNET18 (가중치O)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m resnet \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241m.\u001b[39mresnet18()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 전결합층 변경\u001b[39;00m\n\u001b[0;32m      5\u001b[0m resnet\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(in_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m, out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "# 인코딩용 CNN 모델 생성 : RESNET18 (가중치O)\n",
    "\n",
    "resnet = models.resnet18()\n",
    "# 전결합층 변경\n",
    "resnet.fc = nn.Linear(in_features = 512, out_features = 1)\n",
    "\n",
    "# 모델의 합성곱층 가중치 고정 (완전 연결층은 학습시켜야함)\n",
    "for name, param in resnet.named_parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in resnet.fc.named_parameters():\n",
    "    param.requires_grad = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pack_padded_sequence\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 패딩된 시퀀스를 실제 데이터 길이에 맞게 패킹하여 효율적인 연산을 수행할 수 있게 해줌\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mdecoder\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_size, hidden_size, vocab_size, num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28msuper\u001b[39m(decoder, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# 디코딩용 RNN 모델 생성 : LSTM\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "# 패딩된 시퀀스를 실제 데이터 길이에 맞게 패킹하여 효율적인 연산을 수행할 수 있게 해줌\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers = 1):\n",
    "        super(decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim = embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, features, captions, lengths):\n",
    "        embedding = self.embed(captions)\n",
    "        print(f\"embedding : {embedding.shape}\")\n",
    "        print(f\"features : {features.shape}\")\n",
    "\n",
    "        print(f\"features.unsqueeze(1) : {features.unsqueeze(1).shape}\")\n",
    "        embedding = torch.cat((features.unsqueeze(1), embedding),1)\n",
    "        \n",
    "        packed = pack_padded_sequence(embedding, lengths, batch_first = True)\n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위한 하이퍼 파라미터 정의\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = 21652  # 단어사전 크기\n",
    "num_layers = 1\n",
    "model = decoder(embed_size, hidden_size, 21653, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image, caption in train_dl:\n",
    "#     print(caption[0])\n",
    "#     a = np.array(caption[0])\n",
    "\n",
    "#     print(len(np.nonzero(a)[0]))\n",
    "#     num_list = []\n",
    "#     for ind in range(len(image)):\n",
    "#         a = np.array(caption[ind])\n",
    "#         num_list.append(len(np.nonzero(a)[0]))\n",
    "#     print(num_list)\n",
    "#     break\n",
    "\n",
    "# torch.Tensor(num_list), len(num_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습, 검증, 테스트 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     35\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 38\u001b[0m training(\u001b[43mtrain_dl\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dl' is not defined"
     ]
    }
   ],
   "source": [
    "import torchmetrics.functional as metrics\n",
    "\n",
    "def training(dataloader):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1score_list = []\n",
    "\n",
    "    for images, captions in dataloader:        \n",
    "        num_list = []\n",
    "        for ind in range(len(images)):\n",
    "            a = np.array(captions[ind])\n",
    "            num_list.append(len(np.nonzero(a)[0]))\n",
    "    \n",
    "        lengths = torch.Tensor(num_list)     #배치 사이즈가 아니고 0이 아닌 것의 개수 \n",
    "\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first = True, enforce_sorted=False)[0]\n",
    "\n",
    "        # 이미지 특성 추출\n",
    "        features = resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        # 예측\n",
    "        # print(f\"features {features.shape}, captions {captions.shape}, lengths {lengths.shape}\")\n",
    "\n",
    "        outputs = model(features, captions, lengths)\n",
    "        print(outputs)\n",
    "\n",
    "        # 역전파\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "training(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(dataloader):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1score_list = []\n",
    "\n",
    "    for images, captions in dataloader:        \n",
    "        num_list = []\n",
    "        for ind in range(len(image)):\n",
    "            a = np.array(caption[ind])\n",
    "            num_list.append(len(np.nonzero(a)[0]))\n",
    "    \n",
    "        lengths = torch.Tensor(num_list)     #배치 사이즈가 아니고 0이 아닌 것의 개수 \n",
    "        \n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first = True, enforce_sorted=False)[0]\n",
    "\n",
    "        # 이미지 특성 추출\n",
    "        features = resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        # 예측\n",
    "        outputs = model(features, captions, lengths)\n",
    "        print(outputs)\n",
    "        # 역전파\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 정확도\n",
    "        acc = metrics.accuracy(outputs, outputs, task = 'multiclass') \n",
    "        precision = metrics.precision(outputs, outputs, task = 'multiclass')\n",
    "        recall = metrics.recall(outputs, outputs, task = 'multiclass')\n",
    "        f1score = metrics.f1_score(outputs, outputs, task = 'multiclass')\n",
    "        \n",
    "        loss_list.append(loss)\n",
    "        acc_list.append(acc)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1score_list.append(f1score)\n",
    "        \n",
    "    total_loss = sum(loss_list) / len(loss_list)\n",
    "    total_acc = sum(acc_list) / len(acc_list)\n",
    "    total_precision = sum(precision_list) / len(precision_list)\n",
    "    total_recall = sum(recall_list)/len(recall_list)\n",
    "    total_f1score = sum(f1score_list) / len(f1score_list)\n",
    "    print(f\"{total_loss} {total_acc} {total_precision} {total_recall} {total_f1score}\")\n",
    "    return total_loss, total_acc, total_precision, total_recall, total_f1score\n",
    "\n",
    "training(train_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
