{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchinfo import summary\n",
    "import torchvision.models as models\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩용 CNN 모델 생성 : RESNET18 (가중치O)\n",
    "\n",
    "# res_model = models.resnet18(weights = ( \"ResNet18_Weights.DEFAULT\"))\n",
    "# 전결합층 변경\n",
    "# res_model.fc = nn.Linear(in_features = 512, out_features = 1)\n",
    "\n",
    "\n",
    "class encoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(encoderCNN, self).__init_()\n",
    "        self.resnet = models.resnet18(weights = ( \"ResNet18_Weights.DEFAULT\"))\n",
    "        # 전이학습 모델의 전결합층 변경\n",
    "        self.resent.fc = nn.Linear(self.resenet.fc.in_features, embed_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)  # 흠,, 필요할까?\n",
    "        self.relu = nn.Relu()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.resnet(x)\n",
    "        \n",
    "        # 모델의 합성곱층 가중치 고정 (완전 연결층은 학습시켜야함)\n",
    "        for name, param in self.resnet.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        for name, param in self.resnet.fc.named_parameters():\n",
    "            param.requires_grad = True \n",
    "\n",
    "        result = self.relu(features)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코딩용 RNN 모델 생성 : LSTM\n",
    "\n",
    "class decoderRNN(nn.Module):\n",
    "    def __init__ (self, embed_size, vocab_size, hidden_size, num_layers):\n",
    "        super(decoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5) # 흠 필요함?\n",
    "    \n",
    "    def forward(self, features, caption):\n",
    "        # embeddings = self.dropout(self.embedding(caption))   # 왜 dropout?\n",
    "        embeddings = self.embedding(caption)\n",
    "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim = 0)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 과 RNN을 연결시키자\n",
    "\n",
    "class CNN2RNN(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n",
    "        super(CNN2RNN, self).__init__()\n",
    "        self.encoderCNN = encoderCNN(embed_size)\n",
    "        self.decoderRNN = decoderRNN(embed_size, vocab_size, hidden_size, num_layers)\n",
    "    \n",
    "    def captionImage(self, image, vocabulary, maxlength = 50):\n",
    "        result_caption = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image).unsqueeze(0)\n",
    "            states = None\n",
    "\n",
    "            for _ in range(maxlength) :\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "                print(predicted.shape)\n",
    "\n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embedding(output).unsqueeze(0)\n",
    "\n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\" :\n",
    "                    break\n",
    "        \n",
    "        return [vocabulary.itos[i] for i in result_caption]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        \n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        \n",
    "        self.freq_threshold = freq_threshold\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "    \n",
    "    def build_vocabulary(self,sentences):\n",
    "        idx = 4\n",
    "        frequency = {}\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                if word not in frequency:\n",
    "                    frequency[word] = 1\n",
    "                else:\n",
    "                    frequency[word] += 1\n",
    "                \n",
    "                if (frequency[word] > self.freq_threshold-1):\n",
    "                    self.itos[idx] = word\n",
    "                    self.stoi[word] = idx\n",
    "                    idx += 1\n",
    "    \n",
    "    def numericalize(self,sentence):\n",
    "        tokenized_text = self.tokenizer_eng(sentence)\n",
    "        \n",
    "        return [self.stoi[word] if word in self.stoi else self.stoi[\"<UNK>\"] for word in tokenized_text ]\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>Two young guys with shaggy hair look at their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>Two young  white males are outside near many b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>Two men in green shirts are standing in a yard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>A man in a blue shirt standing in a garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>Two friends enjoy time spent together</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image                                            caption\n",
       "0  1000092795.jpg  Two young guys with shaggy hair look at their ...\n",
       "1  1000092795.jpg  Two young  white males are outside near many b...\n",
       "2  1000092795.jpg     Two men in green shirts are standing in a yard\n",
       "3  1000092795.jpg         A man in a blue shirt standing in a garden\n",
       "4  1000092795.jpg              Two friends enjoy time spent together"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation = pd.read_csv(\"./data/encoded_data_v2.csv\", usecols =[1,3])\n",
    "annotation.columns = ['image','caption']\n",
    "annotation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two young guys with shaggy hair look at their hands while hanging out in the yard',\n",
       " 'Two young  white males are outside near many bushes']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation['caption'].tolist()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir=\"./data/flickr30k_images/\", caption_path=\"./data/data_v2.csv\", freq_threshold=5, transform=None, data_length=10000):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "    \n",
    "        self.df = pd.read_csv(caption_path)[:data_length]\n",
    "        \n",
    "        self.captions = self.df['caption']\n",
    "        self.images = self.df['image']\n",
    "        \n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        \n",
    "        print(len(self.captions.tolist()))\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        image = self.images[index]\n",
    "        \n",
    "        img = Image.open(os.path.join(self.root_dir,image)).convert(\"RGB\")\n",
    "        \n",
    "        if (self.transform):\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        \n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        \n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "        return img, torch.tensor(numericalized_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    def __init__(self, pad_value):\n",
    "        self.pad_value = pad_value\n",
    "    \n",
    "    def __call__(self,batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        img = torch.cat(imgs, dim=0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_value)\n",
    "        \n",
    "        return img, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((356, 356)),\n",
    "            transforms.RandomCrop((299, 299)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(root_dir=\"./data/flickr30k_images/\", caption_path=\"./data/data_v2.csv\", transform=transform, batch_size=32, num_workers=8, shuffle=True, pin_memory=True):\n",
    "    dataset = FlickrDataset(root_dir=root_dir,caption_path=caption_path, transform=transform)\n",
    "    pad_value = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    \n",
    "    loader = DataLoader(dataset=dataset, batch_size=32, num_workers=8, shuffle=True, pin_memory=True, collate_fn=MyCollate(pad_value))\n",
    "    \n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loader, dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m, in \u001b[0;36mget_loader\u001b[1;34m(root_dir, caption_path, transform, batch_size, num_workers, shuffle, pin_memory)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_loader\u001b[39m(root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/flickr30k_images/\u001b[39m\u001b[38;5;124m\"\u001b[39m, caption_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/data_v2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mFlickrDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcaption_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaption_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     pad_value \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mstoi[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m     loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mdataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mMyCollate(pad_value))\n",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mFlickrDataset.__init__\u001b[1;34m(self, root_dir, caption_path, freq_threshold, transform, data_length)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir \u001b[38;5;241m=\u001b[39m root_dir\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[43mpandas\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(caption_path)[:data_length]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pandas' is not defined"
     ]
    }
   ],
   "source": [
    "loader, dataset = get_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
